Proceedings O
of O
the O
1st O
Conference O
of O
the O
Asia O
- O
PaciÔ¨Åc O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O

 O
and O
the O
10th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
: O
System O
Demonstrations O
, O
pages O
8‚Äì13 O

 O
December O
4 O
- O
7 O
, O
2020 O
. O
c O

 O
2020 O
Association O
for O
Computational O
Linguistics8AUTONLU O
: O
An O
On O
- O
demand O
Cloud O
- O
based O
Natural O
Language O

 O
Understanding O
System O
for O
Enterprises O

 O
Nham O
Le1,3Tuan O
Manh O
Lai2,3Trung O
Bui3Doo O
Soon O
Kim3 O

 O
1University O
of O
Waterloo O
, O
Ontario O
, O
Canada O

 O
2University O
of O
Illinois O
at O
Urbana O
- O
Champaign O
, O
USA O

 O
3Adobe O
Research O
, O
San O
Jose O
, O
USA O

 O
Abstract O

 O
With O
the O
renaissance O
of O
deep O
learning O
, O
neural O

 O
networks O
have O
achieved O
promising O
results O
on O

 O
many O
natural O
language O
understanding O
( O
NLU O
) O

 O
tasks O
. O
Even O
though O
the O
source O
codes O
of O
many O

 O
neural O
network O
models O
are O
publicly O
available O
, O

 O
there O
is O
still O
a O
large O
gap O
from O
open O
- O
sourced O

 O
models O
to O
solving O
real O
- O
world O
problems O
in O
en- O

 O
terprises O
. O
Therefore O
, O
to O
Ô¨Åll O
this O
gap O
, O
we O
intro- O

 O
duce O
A O
UTONLU O
, O
an O
on O
- O
demand O
cloud O
- O
based O

 O
system O
with O
an O
easy O
- O
to O
- O
use O
interface O
that O
cov- O

 O
ers O
all O
common O
use O
- O
cases O
and O
steps O
in O
devel- O

 O
oping O
an O
NLU O
model O
. O
A O
UTONLU O
has O
sup- O

 O
ported O
many O
product O
teams O
within O
Adobe O
with O

 O
different O
use O
- O
cases O
and O
datasets O
, O
quickly O
deliv- O

 O
ering O
them O
working O
models O
. O
To O
demonstrate O

 O
the O
effectiveness O
of O
A O
UTONLU O
, O
we O
present O

 O
two O
case O
studies O
. O
i O
) O
We O
build O
a O
practical O
NLU O

 O
model O
for O
handling O
various O
image O
- O
editing O
re- O

 O
quests O
in O
Photoshop O
. O
ii O
) O
We O
build O
powerful O

 O
keyphrase O
extraction O
models O
that O
achieve O
state- O

 O
of O
- O
the O
- O
art O
results O
on O
two O
public O
benchmarks O
. O

 O
In O
both O
cases O
, O
end O
users O
only O
need O
to O
write O
a O

 O
small O
amount O
of O
code O
to O
convert O
their O
datasets O

 O
into O
a O
common O
format O
used O
by O
A O
UTONLU O
. O

 O
1 O
Introduction O

 O
In O
recent O
years O
, O
many O
deep O
learning O
methods O
have O

 O
achieved O
impressive O
results O
on O
a O
wide O
range O
of O

 O
tasks O
, O
ranging O
from O
question O
answering O
( O
Seo O
et O
al O
. O
, O

 O
2017 O
; O
Lai O
et O
al O
. O
, O
2018b O
) O
to O
named O
entity O
recogni- O

 O
tion O
( O
NER O
) O
( O
Lin O
et O
al O
. O
, O
2019 O
; O
Jiang O
et O
al O
. O
, O
2019 O
) O
to O

 O
intent O
detection O
and O
slot O
Ô¨Ålling O
( O
Wang O
et O
al O
. O
, O
2018 O
; O

 O
Chen O
et O
al O
. O
, O
2019 O
) O
. O
Even O
though O
the O
source O
codes O
of O

 O
many O
models O
are O
publicly O
available O
, O
going O
from O
an O

 O
open O
- O
sourced O
implementation O
of O
a O
model O
for O
a O
pub- O

 O
lic O
dataset O
to O
a O
production O
- O
ready O
model O
for O
an O
in- O

 O
house O
dataset O
is O
not O
a O
simple O
task O
. O
Furthermore O
, O
in O

 O
an O
enterprise O
, O
only O
few O
engineers O
are O
familiar O
with O

 O
Equal O
contributions O
. O
The O
work O
was O
conducted O
while O
the O

 O
Ô¨Årst O
two O
authors O
interned O
at O
Adobe O
Research.deep O
learning O
research O
and O
frameworks O
. O
There- O

 O
fore O
, O
to O
facilitate O
the O
development O
and O
adoption O
of O

 O
deep O
learning O
models O
within O
Adobe O
, O
we O
introduce O
a O

 O
new O
system O
named O
AUTONLU O
. O
It O
is O
an O
on O
- O
demand O

 O
cloud O
- O
based O
system O
that O
enables O
multiple O
users O
to O

 O
create O
and O
edit O
datasets O
and O
to O
train O
and O
test O
dif- O

 O
ferent O
state O
- O
of O
- O
the O
- O
art O
NLU O
models O
. O
AUTONLU O
‚Äôs O

 O
main O
principles O
are O
: O

 O
Ease O
of O
use O
.AUTONLU O
aims O
to O
help O
users O

 O
with O
limited O
technical O
knowledge O
to O
train O
and O

 O
test O
models O
on O
their O
datasets O
. O
We O
provide O
GUI O

 O
modules O
to O
accommodate O
the O
most O
common O

 O
use O
- O
cases O
, O
from O
creating O
/ O
cleaning O
a O
dataset O
to O

 O
training O
/ O
evaluating O
/ O
debugging O
a O
model O
. O

 O
State O
- O
of O
- O
the O
- O
art O
models O
. O
Users O
should O
not O

 O
sacriÔ¨Åce O
performance O
for O
ease O
- O
of O
- O
use O
. O
Our O

 O
built O
- O
in O
models O
provide O
state O
- O
of O
- O
the O
- O
art O
per- O

 O
formance O
on O
multiple O
public O
datasets O
. O
AU- O

 O
TONLU O
also O
supports O
hyperparameter O
tuning O

 O
using O
grid O
search O
, O
allowing O
users O
to O
Ô¨Åne O
- O
tune O

 O
the O
models O
even O
further O
. O

 O
Scalability O
.AUTONLU O
aims O
to O
be O
deployed O

 O
in O
enterprises O
where O
computing O
costs O
could O
be O

 O
a O
limiting O
factor O
. O
We O
provide O
an O
on O
- O
demand O
ar- O

 O
chitecture O
so O
that O
the O
system O
could O
be O
utilized O

 O
as O
much O
as O
possible O
. O

 O
At O
Adobe O
, O
AUTONLU O
has O
been O
used O
to O
train O

 O
NLU O
models O
for O
different O
product O
teams O
, O
ranging O

 O
from O
Photoshop O
to O
Document O
Cloud O
. O
To O
demon- O

 O
strate O
the O
effectiveness O
of O
AUTONLU O
, O
we O
present O

 O
two O
case O
studies O
. O
i O
) O
We O
build O
a O
practical O
NLU O

 O
model O
for O
handling O
various O
image O
- O
editing O
requests O

 O
in O
Photoshop O
. O
ii O
) O
We O
build O
powerful O
keyphrase O
ex- O

 O
traction O
models O
that O
achieve O
state O
- O
of O
- O
the O
- O
art O
results O

 O
on O
two O
public O
benchmarks O
. O
In O
both O
cases O
, O
end O
users O

 O
only O
need O
to O
write O
a O
small O
amount O
of O
code O
to O
con- O

 O
vert O
their O
datasets O
into O
a O
common O
format O
used O
by O

 O
AUTONLU.92 O
Related O
work O

 O
Closely O
related O
branches O
of O
work O
to O
ours O
are O
toolk- O

 O
its O
and O
frameworks O
designed O
to O
provide O
a O
suite O
of O

 O
state O
- O
of O
- O
the O
- O
art O
NLP O
models O
to O
users O
( O
Gong O
et O
al O
. O
, O

 O
2019 O
; O
Akbik O
et O
al O
. O
, O
2019 O
; O
Wang O
et O
al O
. O
, O
2019 O
; O
Zhu O

 O
et O
al O
. O
, O
2020 O
; O
Qi O
et O
al O
. O
, O
2020 O
) O
. O
However O
, O
several O

 O
of O
these O
works O
do O
not O
have O
a O
user O
- O
friendly O
inter- O

 O
face O
. O
For O
example O
, O
Flair O
( O
Akbik O
et O
al O
. O
, O
2019 O
) O
, O

 O
NeuronBlocks O
( O
Gong O
et O
al O
. O
, O
2019 O
) O
, O
and O
jiant O

 O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
require O
users O
to O
work O
with O

 O
command O
- O
line O
interfaces O
. O
Different O
from O
these O

 O
works O
, O
an O
end O
- O
user O
with O
no O
programming O
skill O
can O

 O
still O
create O
powerful O
NLU O
models O
using O
our O
sys- O

 O
tem O
. O
Furthermore O
, O
most O
previous O
works O
are O
not O

 O
explicitly O
designed O
for O
enterprise O
settings O
where O

 O
use O
- O
cases O
and O
business O
needs O
can O
be O
vastly O
differ- O

 O
ent O
from O
team O
to O
team O
. O
On O
the O
other O
hand O
, O
since O

 O
AUTONLU O
is O
an O
on O
- O
demand O
cloud O
- O
based O
system O
, O

 O
it O
provides O
more O
Ô¨Çexibility O
to O
end O
users O
. O

 O
In O
2018 O
, O
Google O
introduced O
AutoML O
Natural O

 O
Language1 O
, O
a O
platform O
that O
enables O
users O
to O
build O

 O
and O
deploy O
machine O
learning O
models O
for O
various O

 O
NLP O
tasks O
. O
Our O
system O
is O
different O
from O
AutoML O

 O
in O
the O
following O
aspects O
. O
First O
, O
AutoML O
uses O
neural O

 O
architecture O
search O
( O
NAS O
) O
( O
Elsken O
et O
al O
. O
, O
2019 O
) O
to O

 O
Ô¨Ånd O
the O
best O
model O
for O
the O
task O
of O
interest O
. O
As O
users O

 O
are O
not O
allowed O
to O
simply O
choose O
an O
existing O
archi- O

 O
tecture O
, O
the O
process O
can O
be O
time O
- O
consuming O
even O
for O

 O
simple O
tasks O
( O
e.g. O
, O
2 O
3 O
hours O
) O
. O
On O
the O
other O
hand O
, O

 O
AUTONLU O
provides O
a O
rich O
gallery O
of O
existing O
ar- O

 O
chitectures O
for O
NLU O
. O
In O
future O
work O
, O
we O
are O
also O

 O
planning O
to O
integrate O
NAS O
into O
AUTONLU O
. O
Sec- O

 O
ond O
, O
as O
a O
self O
- O
hosted O
solution O
, O
AUTONLU O
provides O

 O
product O
teams O
of O
Adobe O
with O
total O
control O
over O

 O
their O
datasets O
and O
trained O
models O
. O
This O
enhances O

 O
privacy O
and O
provides O
more O
Ô¨Çexibility O
at O
the O
same O

 O
time O
. O
For O
example O
, O
as O
of O
writing O
, O
there O
is O
no O
way O
to O

 O
download O
a O
trained O
model O
from O
AutoML O
to O
a O
local O

 O
machine O
to O
use O
it O
for O
a O
subsequent O
task O
. O
AUTONLU O

 O
supports O
it O
out O
- O
of O
- O
the O
- O
box O
. O

 O
3 O
A O
UTONLU O

 O
3.1 O
Components O
and O
architecture O

 O
Figure O
1 O
shows O
the O
overall O
architecture O
of O
our O
sys- O

 O
tem O
. O
There O
are O
3 O
main O
components O
: O

 O
A O
web O
application O
that O
serves O
as O
the O
frontend O

 O
to O
the O
users O
. O
The O
most O
important O
component O

 O
of O
the O
application O
is O
a O
Scheduler O
that O
moni- O

 O
1https://cloud.google.com/ O

 O
natural O
- O
language O

 O
Web O
app O

 O
Micro O
server O

 O
Model O

 O
On O
- O
demand O
Cluster O
Scheduler O
Cloud O
Storage O

 O
System O

 O
Model O
Model O
ID O

 O
Test O

 O
input O
Output O
Figure O
1 O
: O
A O
UTONLU O
architecture O
. O
In O
the O
Ô¨Ågure O
is O
the O

 O
dataÔ¨Çow O
when O
the O
user O
calls O
to O
the O
/test O
endpoint O
. O

 O
tors O
the O
status O
of O
the O
cluster O
, O
then O
assigns O
jobs O

 O
to O
the O
most O
appropriate O
instances O
, O
as O
well O
as O

 O
spawns O
more O
/ O
shuts O
off O
instances O
based O
on O
the O

 O
workload O
to O
minimize O
the O
computing O
costs O
. O

 O
The O
user O
interface O
is O
discussed O
in O
more O
detail O

 O
in O
Section O
3.3 O
. O

 O
A O
cloud O
storage O
system O
that O
stores O
datasets O
, O

 O
large O
pre O
- O
trained O
language O
models O
( O
e.g. O
, O
BERT O

 O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
) O
, O
trained O
NLU O
models O
, O

 O
and O
models O
‚Äô O
metadata O
. O
We O
use O
Amazon O
S3 O
as O

 O
our O
storage O
system O
, O
due O
to O
its O
versioning O
sup- O

 O
port O
and O
data O
transfer O
speed O
to O
EC2 O
instances O
. O

 O
An O
on O
- O
demand O
cluster O
that O
performs O
the O
ac- O

 O
tual O
training O
and O
testing O
. O
While O
the O
Lambda O

 O
computing O
model O
seems O
to O
be O
a O
better O
Ô¨Åt O
at O

 O
Ô¨Årst O
thought O
, O
after O
careful O
consideration O
, O
we O

 O
choose O
EC2 O
instances O
to O
prioritize O
user O
ex- O

 O
perience O
over O
some O
costs O
: O
in O
our O
setting O
, O
we O

 O
have O
multiple O
concurrent O
users O
with O
small O
to O

 O
medium O
datasets O
. O
If O
the O
training O
itself O
takes O

 O
only O
10 O
minutes O
, O
any O
amount O
of O
wait O
time O
is O

 O
signiÔ¨Åcant O
. O
By O
maintaining O
a O
certain O
number O

 O
of O
always O
- O
on O
instances O
, O
users O
will O
always O
have O

 O
instant O
interaction O
with O
the O
system O
without O
any O

 O
delay O
. O
Cluster O
‚Äôs O
instances O
are O
initiated O
using O

 O
prebuilt O
images O
, O
which O
we O
discuss O
in O
Section O

 O
3.2 O
. O

 O
3.2 O
Instance O
image O

 O
Regardless O
of O
the O
underlying O
model O
, O
in O
each O
pre- O

 O
built O
image O
, O
an O
included O
webserver O
is O
conÔ¨Ågured O
to O

 O
serve O
the O
following O
endpoints O
: O

 O
/train O
that O
connects O
to O
the O
training O
code O
of O

 O
the O
underlying O
model O
. O

 O
/isfree O
that O
returns O
various O
information O

 O
about O
the O
utilization O
of O
the O
instance O
( O
e.g O
, O
GPU O

 O
memory O
usage O
) O
. O

 O
/test O
that O
connects O
to O
the O
testing O
code O
of O

 O
the O
underlying O
model O
. O

 O
/notebook O
that O
connects O
to O
the O
Jupyter O
Lab O

 O
notebook O
‚Äôs O
URL O
packaged O
in O
the O
image.10 O

 O
Figure O
2 O
: O
Dataset O
view O
of O
A O
UTONLU O
. O

 O
Each O
image O
also O
exposes O
an O
SSH O
connection O
, O
au- O

 O
thenticated O
using O
LDAP O
. O
Experienced O
users O
can O

 O
also O
make O
use O
of O
the O
packaged O
TensorBoard O
to O

 O
monitor O
the O
training O
process O
. O

 O
3.3 O
User O
Interface O

 O
3.3.1 O
Dataset O
Tool O

 O
Public O
and O
internal O
datasets O
come O
in O
many O
different O

 O
formats O
, O
as O
they O
may O
have O
been O
collected O
for O
many O

 O
years O
and O
annotated O
in O
different O
ways O
. O
To O
mitigate O

 O
that O
, O
we O
develop O
an O
intermediate O
representation O

 O
( O
IR O
) O
that O
is O
suitable O
for O
many O
NLU O
tasks O
and O
write O

 O
frontends O
to O
convert O
common O
dataset O
formats O
to O

 O
said O
IR O
. O
We O
also O
provide O
a O
converter O
that O
converts O

 O
this O
IR O
back O
into O
other O
dataset O
formats O
, O
making O

 O
converting O
a O
dataset O
from O
one O
format O
to O
another O

 O
trivial O
. O
In O
our O
setting O
( O
an O
enterprise O
environment O
) O
, O

 O
a O
dataset O
frontend O
converter O
is O
the O
only O
part O
that O

 O
may O
need O
to O
be O
written O
by O
an O
end O
- O
user O
, O
and O
we O

 O
believe O
that O
it O
is O
signiÔ¨Åcantly O
simpler O
than O
building O

 O
the O
whole O
NLU O
pipeline O
. O

 O
Figure O
2 O
shows O
the O
dataset O
view O
. O
Visualizing O
and O

 O
editing O
datapoints O
are O
straightforward O
, O
and O
do O
not O

 O
depend O
on O
the O
source O
/ O
target O
dataset O
format O
( O
Figure O

 O
3 O
) O
. O
While O
it O
is O
not O
common O
to O
edit O
a O
public O
dataset O
, O

 O
the O
same O
is O
typically O
not O
true O
for O
internal O
datasets O
. O

 O
Internal O
datasets O
may O
need O
to O
be O
modiÔ¨Åed O
and O
ex- O

 O
panded O
based O
on O
business O
needs O
and O
use O
- O
cases O
. O

 O
3.3.2 O
Analysis O
Tool O

 O
We O
include O
TensorBoard O
in O
our O
prebuilt O
images O
to O

 O
display O
common O
training O
metrics O
. O
However O
, O
since O

 O
our O
main O
users O
are O
typically O
product O
teams O
with O

 O
limited O
experience O
in O
machine O
learning O
, O
we O
also O

 O
develop O
interactive O
views O
to O
analyze O
the O
trained O
re- O

 O
sults O
. O
For O
example O
, O
Figure O
4 O
shows O
our O
interactive O

 O
confusion O
matrix O
view O
: O
rather O
than O
just O
knowing O

 O
that O
there O
are O
14 O
instances O
in O
which O
a O
mention O
with O

 O
Figure O
3 O
: O
Edit O
/ O
Add O
a O
datapoint O
. O

 O
Figure O
4 O
: O
An O
example O
interactive O
confusion O
matrix O
. O

 O
the O
label O
‚Äú O
Person O
‚Äù O
is O
misclassiÔ¨Åed O
as O
‚Äú O
Location O
‚Äù O
, O

 O
users O
can O
click O
on O
a O
cell O
in O
the O
matrix O
to O
see O
which O

 O
instances O
are O
misclassiÔ¨Åed O
. O
This O
is O
even O
more O
im- O

 O
portant O
for O
internal O
datasets O
: O
the O
errors O
may O
actually O

 O
be O
in O
the O
dataset O
instead O
of O
the O
model O
, O
and O
we O
can O

 O
catch O
it O
using O
this O
view O
. O
In O
fact O
, O
as O
we O
will O
demon- O

 O
strate O
in O
Section O
4.1 O
, O
we O
have O
caught O
many O
labeling O

 O
errors O
in O
our O
internal O
datasets O
using O
this O
tool O
. O

 O
3.3.3 O
Resource O
Management O
Tool O

 O
In O
most O
use O
- O
cases O
, O
AUTONLU O
automatically O
han- O

 O
dles O
resource O
management O
for O
the O
users O
. O
However O
, O

 O
if O
an O
advanced O
user O
wants O
to O
manually O
manage O

 O
instances O
‚Äô O
life O
cycle O
, O
assign O
a O
task O
to O
a O
speciÔ¨Åc O
in- O

 O
stance O
, O
or O
to O
debug O
an O
instance O
, O
we O
provide O
a O
GUI O

 O
to O
do O
so O
as O
well O
. O
Concretely O
, O
we O
provide O
the O
follow- O

 O
ing O
functionalities O
: O

 O
Create O
an O
instance O
with O
a O
desired O
hardware O

 O
conÔ¨Åguration O
and O
docker O
image O
. O
By O
default O
, O

 O
AUTONLU O
creates O
an O
instance O
with O
4 O
CPU O

 O
cores O
, O
8 O
GBs O
of O
RAM O
, O
and O
1 O
NVIDIA O
V100 O

 O
GPU O
, O
which O
are O
all O
conÔ¨Ågurable O
to O
the O
user O
‚Äôs O

 O
desire O
. O
The O
default O
docker O
image O
is O
the O
one O

 O
containing O
all O
the O
supported O
models O
, O
but O
users O

 O
can O
choose O
from O
one O
of O
the O
prebuilt O
images11that O
contains O
just O
a O
single O
model O
if O
that O
‚Äôs O
their O

 O
use O
- O
case O
. O

 O
Assign O
a O
task O
to O
an O
instance O
. O
During O
training O

 O
and O
testing O
, O
users O
can O
choose O
whether O
to O
let O

 O
AUTONLU O
to O
distribute O
the O
task O
or O
to O
assign O

 O
the O
task O
to O
a O
speciÔ¨Åc O
instance O
: O
it O
is O
common O

 O
for O
a O
product O
team O
to O
reserve O
a O
few O
instances O

 O
for O
themselves O
and O
want O
to O
use O
just O
those O
in- O

 O
stances O
. O

 O
Access O
an O
instance O
‚Äôs O
shell O
and O
Ô¨Åles O
. O
Since O

 O
Ease O
- O
of O
- O
use O
is O
one O
of O
our O
core O
design O
princi- O

 O
ples O
, O
we O
package O
in O
all O
of O
our O
prebuilt O
images O

 O
a O
Jupyter O
Lab O
server O
, O
with O
the O
intention O
of O
us- O

 O
ing O
it O
as O
a O
lightweight O
IDE O
/ O
shell O
environment O
. O

 O
While O
we O
also O
expose O
SSH O
connection O
to O
each O

 O
instance O
, O
we O
expect O
users O
to O
Ô¨Ånd O
the O
Jupyter O

 O
Lab O
a O
more O
friendly O
approach O
. O

 O
4 O
Case O
studies O

 O
4.1 O
NLU O
Models O
for O
Image O
- O
Editing O
Requests O

 O
One O
of O
the O
Ô¨Årst O
clients O
of O
AUTONLU O
was O
the O

 O
Photoshop O
team O
, O
as O
we O
want O
to O
build O
a O
chat- O

 O
bot O
using O
their O
image O
- O
editing O
requests O
dataset O

 O
( O
Manuvinakurike O
et O
al O
. O
, O
2018 O
; O
Brixey O
et O
al O
. O
, O
2018 O
) O
. O

 O
The O
dataset O
was O
collected O
in O
many O
years O
, O
annotated O

 O
both O
using O
Amazon O
Mechanical O
Turk O
and O
by O
our O

 O
in O
- O
house O
annotators O
. O
Cleaning O
this O
dataset O
is O
a O
chal- O

 O
lenge O
in O
itself O
, O
and O
in O
this O
case O
study O
, O
we O
aim O
to O

 O
create O
an O
effective O
workÔ¨Çow O
to O
train O
a O
state O
- O
of O
- O
the- O

 O
art O
model O
and O
clean O
the O
dataset O
at O
the O
same O
time O
. O

 O
We O
Ô¨Årst O
convert O
the O
dataset O
into O
our O
IR O
, O
and O
train O

 O
a O
simple O
model O
using O
the O
fastest O
algorithm O
provided O

 O
byAUTONLU O
. O
This O
initial O
model O
provides O
us O
with O

 O
a O
rough O
confusion O
matrix O
, O
and O
we O
manually O
inspect O

 O
cells O
with O
the O
biggest O
values O
. O
Those O
cells O
give O
us O
an O

 O
insight O
into O
some O
systematic O
labeling O
errors O
, O
such O

 O
as O
in O
Figure O
5 O
. O
We O
then O
Ô¨Åx O
those O
labeling O
errors O
, O

 O
either O
by O
using O
the O
dataset O
interface O
in O
AUTONLU O

 O
, O
or O
by O
writing O
scripts O
. O
With O
this O
new O
dataset O
, O
we O

 O
retrain O
another O
model O
and O
repeat O
the O
process O
. O

 O
Once O
the O
fast O
model O
performance O
is O
comparable O

 O
to O
its O
performance O
on O
some O
public O
datasets O
, O
such O

 O
as O
ATIS O
( O
Hemphill O
et O
al O
. O
, O
1990 O
) O
, O
we O
switch O
to O
train O

 O
and O
Ô¨Åne O
- O
tune O
a O
bigger O
model O
. O
More O
speciÔ¨Åcally O
, O
we O

 O
employ O
a O
joint O
intent O
classiÔ¨Åcation O
and O
slot O
Ô¨Ålling O

 O
model O
based O
on O
BERT O
( O
Chen O
et O
al O
. O
, O
2019 O
) O
, O
which O

 O
is O
already O
implemented O
in O
AUTONLU O
. O
By O
the O
end O

 O
of O
this O
process O
, O
we O
end O
up O
with O
a O
powerful O
NLU O

 O
model O
, O
as O
reported O
in O
Table O
1 O
, O
and O
a O
cleaned O
dataset O

 O
that O
is O
useful O
for O
subsequent O
tasks O
. O
The O
NLU O
model O

 O
created O
using O
AUTONLU O
outperforms O
a O
compet O
- O
True O
l O
a O
b O
e O
l O
: O
B O
 a O
d O
j O
u O
s O
t O
b O
r O
i O
g O
h O
t O
n O
e O
s O
s O

 O
Pred O
l O
a O
b O
e O
l O
: O
B O
 a O
d O
j O
u O
s O
t O
c O
o O
l O
o O
r O

 O
[ O
[ O
CLS O
] O
l O
i O
g O
h O
t O
# O
# O
en O
t O
h O
e O
v O
e O
g O
e O
t O
a O
b O
l O
e O
s O
[ O
SEP O
] O
] O

 O
[ O
[ O
CLS O
] O
make O
t O
h O
e O
d O
i O
r O
t O
d O
a O
r O
k O
e O
r O
i O
n O
brown O

 O
c O
o O
l O
o O
r O
[ O
SEP O
] O
] O

 O
Figure O
5 O
: O
2 O
labeling O
errors O
captured O
by O
the O
interactive O

 O
confusion O
matrix O
near O
the O
end O
of O
the O
training O
- O
cleaning O

 O
process O
. O
The O
# O
# O
is O
the O
artifact O
from O
BERT O
tokenizer O
. O

 O
ModelMetrics O

 O
Intent O
SP O
SR O
SF1 O

 O
JIS O
( O
2016 O
) O
0.832 O
0.850 O
0.726 O
0.783 O

 O
RASA O
0.924 O
0.833 O
0.605 O
0.701 O

 O
AUTONLU O
0.954 O
0.869 O
0.854 O
0.862 O

 O
Table O
1 O
: O
Results O
on O
the O
image O
- O
editing O
requests O
dataset O
. O

 O
Intent O
accuracy O
, O
slot O
precision O
, O
slot O
recall O
, O
and O
slot O
F1 O

 O
scores O
are O
reported O
. O
Scores O
of O
our O
models O
are O
averaged O

 O
over O
three O
random O
seeds O
. O

 O
ing O
model O
created O
using O
RASA O
( O
Bocklisch O
et O
al O
. O
, O

 O
2017 O
) O
and O
a O
joint O
model O
of O
intent O
determination O

 O
and O
slot O
Ô¨Ålling O
( O
JIS O
) O
( O
Zhang O
and O
Wang O
, O
2016 O
) O
by O
a O

 O
large O
margin O
. O

 O
4.2 O
Keyphrase O
Extraction O
Models O

 O
Keyphrase O
extraction O
is O
the O
task O
of O
automatically O

 O
extracting O
a O
small O
set O
of O
phrases O
that O
best O
describe O

 O
a O
document O
. O
As O
keyphrases O
provide O
a O
high O
- O
level O

 O
summarization O
of O
the O
considered O
document O
and O

 O
they O
give O
the O
reader O
some O
clues O
about O
its O
contents O
, O

 O
keyphrase O
extraction O
is O
a O
problem O
of O
great O
interest O

 O
to O
the O
Document O
Cloud O
team O
of O
Adobe O
. O
In O
this O
case O

 O
study O
, O
we O
aim O
to O
develop O
an O
effective O
keyphrase O

 O
extraction O
system O
for O
the O
team O
. O

 O
Similar O
to O
recent O
works O
on O
keyphrase O
extraction O

 O
( O
Sahrawat O
et O
al O
. O
, O
2020 O
) O
, O
we O
formulate O
the O
task O
as O
a O

 O
sequence O
labeling O
task O
. O
Given O
an O
input O
sequence O
of O

 O
tokens O
x O
= O
fx1 O
; O
x2 O
; O
: O
: O
: O
; O
x O
ng O
, O
the O
goal O
is O
to O
predict O

 O
a O
sequence O
of O
labels O
y O
= O
fy1 O
; O
y2 O
; O
: O
: O
: O
; O
y O
ngwhere O

 O
yi2fB;I;Og O
. O
Here O
, O
label O
Bdenotes O
the O
begin- O

 O
ning O
of O
a O
keyphrase O
, O
Idenotes O
the O
continuation O

 O
of O
a O
keyphrase O
, O
and O
Ocorresponds O
to O
tokens O
that O

 O
are O
not O
part O
of O
any O
keyphrase O
. O
This O
formulation O
is O

 O
naturally O
supported O
by O
our O
platform O
, O
as O
the O
task O
of O

 O
slot O
Ô¨Ålling O
in O
NLU O
is O
basically O
a O
sequence O
label- O

 O
ing O
task O
. O
We O
Ô¨Årst O
collect O
two O
public O
datasets O
for O

 O
keyphrase O
extraction O
: O
Inspec O
( O
Hulth O
, O
2003 O
) O
and O
SE- O

 O
2017 O
( O
Augenstein O
et O
al O
. O
, O
2017 O
) O
. O
We O
then O
convert O

 O
them O
to O
the O
common O
intermediate O
representation O
. O

 O
After O
that O
, O
we O
simply O
use O
AUTONLU O
to O
train O
and O

 O
tune O
models O
. O
We O
employ O
the O
BiLSTM O
- O
CRF O
archi-12ModelDatasets O

 O
Inspec O
SE-2017 O

 O
KEA O
( O
2005 O
) O
0.137 O
0.129 O

 O
TextRank O
( O
2004 O
) O
0.122 O
0.157 O

 O
SingeRank O
( O
2008 O
) O
0.123 O
0.155 O

 O
SGRank O
( O
2015 O
) O
0.271 O
0.211 O

 O
Transformer O
( O
2020 O
) O
0.595 O
0.522 O

 O
BERT O
( O
A O
UTONLU O
) O
0.596 O
0.537 O

 O
SciBERT O
( O
A O
UTONLU O
) O
0.598 O
0.544 O

 O
Table O
2 O
: O
Results O
on O
Inspec O
and O
SE-2017 O
datasets O
. O
F1 O

 O
scores O
are O
reported O
. O
Scores O
of O
our O
models O
are O
averaged O

 O
over O
three O
random O
seeds O
. O

 O
tecture O
( O
Huang O
et O
al O
. O
, O
2015 O
) O
that O
is O
already O
available O

 O
inAUTONLU O
. O
We O
experiment O
with O
two O
different O

 O
pre O
- O
trained O
language O
models O
as O
the O
Ô¨Årst O
embedding O

 O
layer O
: O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
and O
SciBERT O

 O
( O
Beltagy O
et O
al O
. O
, O
2019 O
) O
. O
Table O
2 O
shows O
the O
results O
on O

 O
the O
datasets O
. O
We O
see O
that O
both O
models O
created O
us- O

 O
ingAUTONLU O
outperform O
previous O
models O
for O
the O

 O
task O
, O
achieving O
new O
state O
- O
of O
- O
the O
- O
art O
results O
. O
As O
AU- O

 O
TONLU O
can O
automatically O
perform O
hyperparame- O

 O
ter O
tuning O
using O
grid O
search O
, O
models O
produced O
by O

 O
AUTONLU O
typically O
have O
satisfying O
performance O

 O
( O
assuming O
that O
the O
selected O
underlying O
architecture O

 O
is O
expressive O
enough O
) O
. O
It O
is O
worth O
noting O
that O
during O

 O
this O
entire O
process O
, O
the O
only O
code O
we O
need O
to O
write O

 O
is O
for O
converting O
the O
Inspec O
and O
SE-2017 O
datasets O

 O
to O
the O
IR O
. O

 O
5 O
Conclusion O

 O
In O
this O
work O
, O
we O
introduce O
AUTONLU O
, O
an O
on- O

 O
demand O
cloud O
- O
based O
platform O
that O
is O
easy O
- O
to O
- O
use O

 O
and O
has O
enabled O
many O
product O
teams O
within O
Adobe O

 O
to O
create O
powerful O
NLU O
models O
. O
Our O
design O
princi- O

 O
ples O
make O
it O
an O
ideal O
candidate O
for O
enterprises O
who O

 O
want O
to O
have O
an O
NLU O
system O
for O
themselves O
, O
with O

 O
minimal O
deep O
learning O
expertise O
. O
AUTONLU O
‚Äôs O

 O
code O
is O
in O
the O
process O
to O
be O
open O
- O
sourced O
, O
and O
we O

 O
invite O
contributors O
to O
contribute O
. O
In O
future O
work O
, O

 O
we O
will O
implement O
more O
advanced O
features O
such O
as O

 O
transfer O
learning O
, O
knowledge O
distillation O
and O
neu- O

 O
ral O
architecture O
search O
, O
which O
have O
been O
shown O
to O

 O
be O
useful O
in O
building O
real O
- O
world O
NLP O
systems O
( O
Lai O

 O
et O
al O
. O
, O
2018a O
; O
Jiang O
et O
al O
. O
, O
2019 O
; O
Lai O
et O
al O
. O
, O
2019 O
, O

 O
2020 O
; O
Klyuchnikov O
et O
al O
. O
, O
2020 O
) O
. O
Furthermore O
, O
we O

 O
will O
extend O
our O
system O
to O
have O
more O
advanced O
ana- O

 O
lytics O
features O
( O
Murugesan O
et O
al O
. O
, O
2019 O
) O
, O
and O
to O
bet- O

 O
ter O
support O
other O
languages O
( O
Nguyen O
and O
Nguyen O
, O

 O
2020).References O

 O
A. O
Akbik O
, O
T. O
Bergmann O
, O
Duncan O
Blythe O
, O
K. O
Rasul O
, O
Ste- O

 O
fan O
Schweter O
, O
and O
Roland O
V O
ollgraf O
. O
2019 O
. O
Flair O
: O
An O

 O
easy O
- O
to O
- O
use O
framework O
for O
state O
- O
of O
- O
the O
- O
art O
nlp O
. O
In O

 O
NAACL O
- O
HLT O
. O

 O
Isabelle O
Augenstein O
, O
Mrinal O
Das O
, O
Sebastian O
Riedel O
, O

 O
Lakshmi O
Vikraman O
, O
and O
Andrew O
McCallum O
. O
2017 O
. O

 O
Semeval O
2017 O
task O
10 O
: O
Scienceie O
- O
extracting O

 O
keyphrases O
and O
relations O
from O
scientiÔ¨Åc O
publications O
. O

 O
CoRR O
, O
abs/1704.02853 O
. O

 O
Iz O
Beltagy O
, O
Kyle O
Lo O
, O
and O
Arman O
Cohan O
. O
2019 O
. O
Scibert O
: O

 O
A O
pretrained O
language O
model O
for O
scientiÔ¨Åc O
text O
. O
In O

 O
EMNLP O
/ O
IJCNLP O
. O

 O
Tom O
Bocklisch O
, O
Joey O
Faulkner O
, O
Nick O
Pawlowski O
, O
and O

 O
Alan O
Nichol O
. O
2017 O
. O
Rasa O
: O
Open O
source O
language O

 O
understanding O
and O
dialogue O
management O
. O
ArXiv O
, O

 O
abs/1712.05181 O
. O

 O
Jacqueline O
Brixey O
, O
Ramesh O
Manuvinakurike O
, O
Nham O

 O
Le O
, O
Tuan O
Lai O
, O
Walter O
Chang O
, O
and O
Trung O
Bui O
. O

 O
2018 O
. O
A O
system O
for O
automated O
image O
editing O

 O
from O
natural O
language O
commands O
. O
arXiv O
preprint O

 O
arXiv:1812.01083 O
. O

 O
Qian O
Chen O
, O
Zhu O
Zhuo O
, O
and O
Wen O
Wang O
. O
2019 O
. O
Bert O

 O
for O
joint O
intent O
classiÔ¨Åcation O
and O
slot O
Ô¨Ålling O
. O
ArXiv O
, O

 O
abs/1902.10909 O
. O

 O
Soheil O
Danesh O
, O
Tamara O
Sumner O
, O
and O
James O
H. O
Martin O
. O

 O
2015 O
. O
Sgrank O
: O
Combining O
statistical O
and O
graphi- O

 O
cal O
methods O
to O
improve O
the O
state O
of O
the O
art O
in O
unsu- O

 O
pervised O
keyphrase O
extraction O
. O
In O
* O
SEM@NAACL- O

 O
HLT O
. O

 O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O

 O
Kristina O
Toutanova O
. O
2018 O
. O
BERT O
: O
pre O
- O
training O
of O

 O
deep O
bidirectional O
transformers O
for O
language O
under- O

 O
standing O
. O
CoRR O
, O
abs/1810.04805 O
. O

 O
Thomas O
Elsken O
, O
Jan O
Hendrik O
Metzen O
, O
and O
Frank O
Hutter O
. O

 O
2019 O
. O
Neural O
architecture O
search O
: O
A O
survey O
. O
ArXiv O
, O

 O
abs/1808.05377 O
. O

 O
Ming O
Gong O
, O
Linjun O
Shou O
, O
Wutao O
Lin O
, O
Zhijie O
Sang O
, O

 O
Quanjia O
Yan O
, O
Ze O
Yang O
, O
and O
Daxin O
Jiang O
. O
2019 O
. O
Neu- O

 O
ronblocks O
- O
building O
your O
nlp O
dnn O
models O
like O
play- O

 O
ing O
lego O
. O
ArXiv O
, O
abs/1904.09535 O
. O

 O
Charles O
T. O
Hemphill O
, O
John O
J. O
Godfrey O
, O
and O
George O
R. O

 O
Doddington O
. O
1990 O
. O
The O
ATIS O
spoken O
language O
sys- O

 O
tems O
pilot O
corpus O
. O
In O
Speech O
and O
Natural O
Language O
: O

 O
Proceedings O
of O
a O
Workshop O
Held O
at O
Hidden O
Valley O
, O

 O
Pennsylvania O
, O
June O
24 O
- O
27,1990 O
. O

 O
Zhiheng O
Huang O
, O
Wei O
Xu O
, O
and O
Kai O
Yu O
. O
2015 O
. O
Bidirec- O

 O
tional O
lstm O
- O
crf O
models O
for O
sequence O
tagging O
. O
arXiv O

 O
preprint O
arXiv:1508.01991 O
. O

 O
Anette O
Hulth O
. O
2003 O
. O
Improved O
automatic O
keyword O

 O
extraction O
given O
more O
linguistic O
knowledge O
. O
In O

 O
EMNLP O
.13Yufan O
Jiang O
, O
Chi O
Hu O
, O
Tong O
Xiao O
, O
Chunliang O
Zhang O
, O
and O

 O
Jingbo O
Zhu O
. O
2019 O
. O
Improved O
differentiable O
architec- O

 O
ture O
search O
for O
language O
modeling O
and O
named O
entity O

 O
recognition O
. O
In O
EMNLP O
/ O
ICJNLP O
. O

 O
Nikita O
Klyuchnikov O
, O
Ilya O
TroÔ¨Åmov O
, O
Ekaterina O
Arte- O

 O
mova O
, O
Mikhail O
Salnikov O
, O
Maxim O
Fedorov O
, O
and O

 O
Evgeny O
Burnaev O
. O
2020 O
. O
Nas O
- O
bench O
- O
nlp O
: O
Neural O
ar- O

 O
chitecture O
search O
benchmark O
for O
natural O
language O

 O
processing O
. O
arXiv O
preprint O
arXiv:2006.07116 O
. O

 O
Tuan O
Lai O
, O
Trung O
Bui O
, O
Nedim O
Lipka O
, O
and O
Sheng O
Li O
. O

 O
2018a O
. O
Supervised O
transfer O
learning O
for O
product O
in- O

 O
formation O
question O
answering O
. O
In O
2018 O
17th O
IEEE O

 O
International O
Conference O
on O
Machine O
Learning O
and O

 O
Applications O
( O
ICMLA O
) O
, O
pages O
1109‚Äì1114 O
. O
IEEE O
. O

 O
Tuan O
Lai O
, O
Quan O
Hung O
Tran O
, O
Trung O
Bui O
, O
and O
Daisuke O

 O
Kihara O
. O
2019 O
. O
A O
gated O
self O
- O
attention O
memory O

 O
network O
for O
answer O
selection O
. O
arXiv O
preprint O

 O
arXiv:1909.09696 O
. O

 O
Tuan O
Manh O
Lai O
, O
Trung O
Bui O
, O
and O
Sheng O
Li O
. O
2018b O
. O
A O

 O
review O
on O
deep O
learning O
techniques O
applied O
to O
an- O

 O
swer O
selection O
. O
In O
Proceedings O
of O
the O
27th O
Inter- O

 O
national O
Conference O
on O
Computational O
Linguistics O
, O

 O
pages O
2132‚Äì2144 O
, O
Santa O
Fe O
, O
New O
Mexico O
, O
USA O
. O
As- O

 O
sociation O
for O
Computational O
Linguistics O
. O

 O
Tuan O
Manh O
Lai O
, O
Quan O
Hung O
Tran O
, O
Trung O
Bui O
, O
and O

 O
Daisuke O
Kihara O
. O
2020 O
. O
A O
simple O
but O
effective O
bert O

 O
model O
for O
dialog O
state O
tracking O
on O
resource O
- O
limited O

 O
systems O
. O
In O
ICASSP O
2020 O
- O
2020 O
IEEE O
International O

 O
Conference O
on O
Acoustics O
, O
Speech O
and O
Signal O
Pro- O

 O
cessing O
( O
ICASSP O
) O
, O
pages O
8034‚Äì8038 O
. O
IEEE O
. O

 O
Ying O
Lin O
, O
Liyuan O
Liu O
, O
Heng O
Ji O
, O
Dong O
Yu O
, O
and O
Jiawei O

 O
Han O
. O
2019 O
. O
Reliability O
- O
aware O
dynamic O
feature O
com- O

 O
position O
for O
name O
tagging O
. O
In O
Proceedings O
of O
the O

 O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Compu- O

 O
tational O
Linguistics O
, O
pages O
165‚Äì174 O
, O
Florence O
, O
Italy O
. O

 O
Association O
for O
Computational O
Linguistics O
. O

 O
Ramesh O
R. O
Manuvinakurike O
, O
Jacqueline O
Brixey O
, O
Trung O

 O
Bui O
, O
W. O
Chang O
, O
Doo O
Soon O
Kim O
, O
Ron O
Artstein O
, O
and O

 O
Kallirroi O
Georgila O
. O
2018 O
. O
Edit O
me O
: O
A O
corpus O
and O

 O
a O
framework O
for O
understanding O
natural O
language O
im- O

 O
age O
editing O
. O
In O
LREC O
. O

 O
Rada O
Mihalcea O
and O
Paul O
Tarau O
. O
2004 O
. O
Textrank O
: O
Bring- O

 O
ing O
order O
into O
text O
. O
In O
EMNLP O
. O

 O
Sugeerth O
Murugesan O
, O
Sana O
Malik O
, O
Fan O
Du O
, O
Eunyee O

 O
Koh O
, O
and O
Tuan O
Manh O
Lai O
. O
2019 O
. O
Deepcompare O
: O

 O
Visual O
and O
interactive O
comparison O
of O
deep O
learning O

 O
model O
performance O
. O
IEEE O
computer O
graphics O
and O

 O
applications O
, O
39(5):47‚Äì59 O
. O

 O
Dat O
Quoc O
Nguyen O
and O
Anh O
Tuan O
Nguyen O
. O
2020 O
. O

 O
Phobert O
: O
Pre O
- O
trained O
language O
models O
for O
viet- O

 O
namese O
. O
arXiv O
preprint O
arXiv:2003.00744 O
. O

 O
Peng O
Qi O
, O
Yuhao O
Zhang O
, O
Yuhui O
Zhang O
, O
Jason O
Bolton O
, O

 O
and O
Christopher O
D. O
Manning O
. O
2020 O
. O
Stanza O
: O
A O

 O
python O
natural O
language O
processing O
toolkit O
for O
many O

 O
human O
languages O
. O
In O
ACL.Dhruva O
Sahrawat O
, O
Debanjan O
Mahata O
, O
Haimin O
Zhang O
, O

 O
Mayank O
Kulkarni O
, O
Agniv O
Sharma O
, O
Rakesh O
Gosangi O
, O

 O
Amanda O
Stent O
, O
Yaman O
Kumar O
, O
Rajiv O
Ratn O
Shah O
, O
and O

 O
Roger O
Zimmermann O
. O
2020 O
. O
Keyphrase O
extraction O
as O

 O
sequence O
labeling O
using O
contextualized O
embeddings O
. O

 O
InEuropean O
Conference O
on O
Information O
Retrieval O
, O

 O
pages O
328‚Äì335 O
. O
Springer O
. O

 O
Minjoon O
Seo O
, O
Aniruddha O
Kembhavi O
, O
Ali O
Farhadi O
, O
and O

 O
Hannaneh O
Hajishirzi O
. O
2017 O
. O
Bidirectional O
atten- O

 O
tion O
Ô¨Çow O
for O
machine O
comprehension O
. O
ArXiv O
, O

 O
abs/1611.01603 O
. O

 O
Xiaojun O
Wan O
and O
Jianguo O
Xiao O
. O
2008 O
. O
Single O
doc- O

 O
ument O
keyphrase O
extraction O
using O
neighborhood O

 O
knowledge O
. O
In O
AAAI O
. O

 O
Alex O
Wang O
, O
Ian O
F. O
Tenney O
, O
Yada O
Pruksachatkun O
, O

 O
Katherin O
Yu O
, O
Jan O
Hula O
, O
Patrick O
Xia O
, O
Raghu O
Pappa- O

 O
gari O
, O
Shuning O
Jin O
, O
R. O
Thomas O
McCoy O
, O
Roma O
Pa- O

 O
tel O
, O
Yinghui O
Huang O
, O
Jason O
Phang O
, O
Edouard O
Grave O
, O

 O
Haokun O
Liu O
, O
Najoung O
Kim O
, O
Phu O
Mon O
Htut O
, O
Thibault O

 O
F‚Äôevry O
, O
Berlin O
Chen O
, O
Nikita O
Nangia O
, O
Anhad O
Mo- O

 O
hananey O
, O
Katharina O
Kann O
, O
Shikha O
Bordia O
, O
Nicolas O

 O
Patry O
, O
David O
Benton O
, O
Ellie O
Pavlick O
, O
and O
Samuel O
R. O

 O
Bowman O
. O
2019 O
. O
jiant O
1.2 O
: O
A O
software O
toolkit O

 O
for O
research O
on O
general O
- O
purpose O
text O
understanding O

 O
models O
. O
http://jiant.info/ O
. O

 O
Yu O
Wang O
, O
Yilin O
Shen O
, O
and O
Hongxia O
Jin O
. O
2018 O
. O
A O

 O
bi O
- O
model O
based O
rnn O
semantic O
frame O
parsing O
model O

 O
for O
intent O
detection O
and O
slot O
Ô¨Ålling O
. O
ArXiv O
, O

 O
abs/1812.10235 O
. O

 O
Ian O
H O
Witten O
, O
Gordon O
W O
Paynter O
, O
Eibe O
Frank O
, O
Carl O

 O
Gutwin O
, O
and O
Craig O
G O
Nevill O
- O
Manning O
. O
2005 O
. O
Kea O
: O

 O
Practical O
automated O
keyphrase O
extraction O
. O
In O
Design O

 O
and O
Usability O
of O
Digital O
Libraries O
: O
Case O
Studies O
in O

 O
the O
Asia O
PaciÔ¨Åc O
, O
pages O
129‚Äì152 O
. O
IGI O
global O
. O

 O
Xiaodong O
Zhang O
and O
Houfeng O
Wang O
. O
2016 O
. O
A O
joint O

 O
model O
of O
intent O
determination O
and O
slot O
Ô¨Ålling O
for O
spo- O

 O
ken O
language O
understanding O
. O
In O
IJCAI O
. O

 O
Qi O
Zhu O
, O
Zheng O
Zhang O
, O
Yan O
Fang O
, O
Xiang O
Li O
, O
Ryuichi O

 O
Takanobu O
, O
Jin O
chao O
Li O
, O
Baolin O
Peng O
, O
Jianfeng O
Gao O
, O

 O
Xiao O
- O
Yan O
Zhu O
, O
and O
Minlie O
Huang O
. O
2020 O
. O
Convlab- O

 O
2 O
: O
An O
open O
- O
source O
toolkit O
for O
building O
, O
evaluat- O

 O
ing O
, O
and O
diagnosing O
dialogue O
systems O
. O
ArXiv O
, O

 O
abs/2002.04793 O
. O
